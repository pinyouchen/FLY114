{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bfb33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:42:04) [MSC v.1943 64 bit (AMD64)]\n",
      "====================================================================================================\n",
      "完整改善版機器學習分析 - 啟動\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# 統計與科學計算\n",
    "try:\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    SCIPY_OK = True\n",
    "except:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "# 機器學習核心\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFECV\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, train_test_split, cross_validate,\n",
    "    GridSearchCV, RandomizedSearchCV, learning_curve\n",
    ")\n",
    "\n",
    "# 模型\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, StackingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 處理不平衡\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# 評估指標\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    precision_recall_curve, roc_curve, confusion_matrix,\n",
    "    classification_report, make_scorer\n",
    ")\n",
    "\n",
    "# 視覺化設定\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# ==================== 全域設定 ====================\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"=\" * 100)\n",
    "print(\"完整改善版機器學習分析 - 啟動\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77819aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路徑設定\n",
    "BASE_DIR = Path(r\"D:\\FLY114\")\n",
    "XLSX_PATH = BASE_DIR / \"Diagnosis and autonomic marker data for VNS research_20250813.xlsx\"\n",
    "\n",
    "# 輸出資料夾\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = BASE_DIR / f\"Analysis_Results_{timestamp}\"\n",
    "PLOTS_DIR = OUT_DIR / \"plots\"\n",
    "MODELS_DIR = OUT_DIR / \"trained_models\"\n",
    "REPORTS_DIR = OUT_DIR / \"reports\"\n",
    "\n",
    "for d in [OUT_DIR, PLOTS_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SHEET_NAME = \"Sheet1\"\n",
    "\n",
    "# 欄位定義\n",
    "BASIC_COLS = [\"Age\", \"Sex\", \"BMI\"]\n",
    "LABEL_COLS = [\"SSD\", \"MDD\", \"Panic\", \"GAD\"]\n",
    "CONTROL_COLS = [\"DM\", \"TCA\", \"MARTA\"]\n",
    "HRV_COLS = [\"SDNN\", \"LF\", \"HF\", \"LFHF\", \"SC\", \"FT\", \"RSA\"]\n",
    "ALL_FEATURES = BASIC_COLS + CONTROL_COLS + HRV_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae69e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 1] 資料載入與清理\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ 原始資料形狀: (502, 17)\n",
      "\n",
      "離群值移除報告:\n",
      "feature  n_outliers    pct\n",
      "   SDNN          24  4.78%\n",
      "     LF          53 10.56%\n",
      "     HF          51 10.16%\n",
      "   LFHF          28  5.58%\n",
      "     SC          19  3.78%\n",
      "     FT           1  0.20%\n",
      "    RSA          25  4.98%\n",
      "    BMI           2  0.40%\n",
      "✓ 清理後資料形狀: (502, 17)\n"
     ]
    }
   ],
   "source": [
    "# ==================== 1. 資料載入與清理 ====================\n",
    "print(\"\\n[步驟 1] 資料載入與清理\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "df = pd.read_excel(XLSX_PATH, sheet_name=SHEET_NAME)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(f\"✓ 原始資料形狀: {df.shape}\")\n",
    "\n",
    "# 型別轉換\n",
    "for c in [*ALL_FEATURES, *LABEL_COLS]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 處理 Sex\n",
    "if \"Sex\" in df.columns and df[\"Sex\"].dtype == object:\n",
    "    norm = df[\"Sex\"].astype(str).str.lower().str[0].map({\"m\": 1, \"f\": 0})\n",
    "    if norm.notna().mean() > 0.6:\n",
    "        df[\"Sex\"] = norm\n",
    "\n",
    "# LFHF >= 0\n",
    "if \"LFHF\" in df.columns:\n",
    "    df.loc[df[\"LFHF\"] < 0, \"LFHF\"] = np.nan\n",
    "\n",
    "# 離群值處理 (使用 IQR 方法)\n",
    "def remove_outliers_iqr(df, cols, factor=3.0):\n",
    "    df_clean = df.copy()\n",
    "    outlier_report = []\n",
    "    \n",
    "    for col in cols:\n",
    "        if col in df_clean.columns:\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - factor * IQR\n",
    "            upper = Q3 + factor * IQR\n",
    "            outliers = ((df_clean[col] < lower) | (df_clean[col] > upper))\n",
    "            n_outliers = outliers.sum()\n",
    "            \n",
    "            if n_outliers > 0:\n",
    "                outlier_report.append({\n",
    "                    'feature': col,\n",
    "                    'n_outliers': n_outliers,\n",
    "                    'pct': f\"{100*n_outliers/len(df):.2f}%\"\n",
    "                })\n",
    "                df_clean.loc[outliers, col] = np.nan\n",
    "    \n",
    "    if outlier_report:\n",
    "        print(\"\\n離群值移除報告:\")\n",
    "        print(pd.DataFrame(outlier_report).to_string(index=False))\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "numerical_cols = [c for c in HRV_COLS + [\"Age\", \"BMI\"] if c in df.columns]\n",
    "df = remove_outliers_iqr(df, numerical_cols, factor=3.0)\n",
    "\n",
    "print(f\"✓ 清理後資料形狀: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89eb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 2] 特徵工程\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ 新增 10 個工程特徵\n",
      "✓ 總特徵數: 23\n"
     ]
    }
   ],
   "source": [
    "# ==================== 2. 特徵工程 ====================\n",
    "print(\"\\n[步驟 2] 特徵工程\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "engineered_features = []\n",
    "\n",
    "# 交互特徵\n",
    "if all(c in df.columns for c in [\"LF\", \"HF\"]):\n",
    "    df[\"LF_HF_ratio\"] = df[\"LF\"] / (df[\"HF\"] + 1e-9)\n",
    "    df[\"LF_plus_HF\"] = df[\"LF\"] + df[\"HF\"]\n",
    "    df[\"LF_minus_HF\"] = df[\"LF\"] - df[\"HF\"]\n",
    "    engineered_features.extend([\"LF_HF_ratio\", \"LF_plus_HF\", \"LF_minus_HF\"])\n",
    "\n",
    "if all(c in df.columns for c in [\"Age\", \"BMI\"]):\n",
    "    df[\"Age_BMI_interaction\"] = df[\"Age\"] * df[\"BMI\"]\n",
    "    df[\"Age_squared\"] = df[\"Age\"] ** 2\n",
    "    engineered_features.extend([\"Age_BMI_interaction\", \"Age_squared\"])\n",
    "\n",
    "if \"SDNN\" in df.columns and \"SC\" in df.columns:\n",
    "    df[\"SDNN_SC_ratio\"] = df[\"SDNN\"] / (df[\"SC\"] + 1e-9)\n",
    "    engineered_features.append(\"SDNN_SC_ratio\")\n",
    "\n",
    "if \"LF\" in df.columns and \"SDNN\" in df.columns:\n",
    "    df[\"LF_SDNN_ratio\"] = df[\"LF\"] / (df[\"SDNN\"] + 1e-9)\n",
    "    engineered_features.append(\"LF_SDNN_ratio\")\n",
    "\n",
    "# 平方根轉換 (處理偏態分佈)\n",
    "for col in [\"HF\", \"LF\", \"SDNN\"]:\n",
    "    if col in df.columns:\n",
    "        df[f\"{col}_sqrt\"] = np.sqrt(df[col].clip(lower=0))\n",
    "        engineered_features.append(f\"{col}_sqrt\")\n",
    "\n",
    "# 更新特徵列表\n",
    "ALL_FEATURES_EXT = ALL_FEATURES + [f for f in engineered_features if f in df.columns]\n",
    "print(f\"✓ 新增 {len(engineered_features)} 個工程特徵\")\n",
    "print(f\"✓ 總特徵數: {len(ALL_FEATURES_EXT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a15597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 3] 缺失值分析\n",
      "----------------------------------------------------------------------------------------------------\n",
      "      feature  missing_count missing_pct\n",
      "  LF_minus_HF             63       12.5%\n",
      "   LF_plus_HF             63       12.5%\n",
      "  LF_HF_ratio             63       12.5%\n",
      "LF_SDNN_ratio             55       11.0%\n",
      "           LF             53       10.6%\n",
      "      LF_sqrt             53       10.6%\n",
      "      HF_sqrt             51       10.2%\n",
      "           HF             51       10.2%\n",
      "SDNN_SC_ratio             43        8.6%\n",
      "         LFHF             28        5.6%\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3. 缺失值分析與處理 ====================\n",
    "print(\"\\n[步驟 3] 缺失值分析\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "missing_report = pd.DataFrame({\n",
    "    'feature': [c for c in ALL_FEATURES_EXT if c in df.columns],\n",
    "    'missing_count': [df[c].isna().sum() for c in ALL_FEATURES_EXT if c in df.columns],\n",
    "    'missing_pct': [f\"{100*df[c].isna().sum()/len(df):.1f}%\" for c in ALL_FEATURES_EXT if c in df.columns]\n",
    "})\n",
    "\n",
    "print(missing_report.sort_values('missing_count', ascending=False).head(10).to_string(index=False))\n",
    "missing_report.to_csv(REPORTS_DIR / \"missing_values_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b8acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 4] 診斷標籤分佈\n",
      "----------------------------------------------------------------------------------------------------\n",
      "label  total  negative  positive pos_ratio imbalance_ratio\n",
      "  SSD    502       378       124     24.7%           1:3.0\n",
      "  MDD    502       402       100     19.9%           1:4.0\n",
      "Panic    502       456        46      9.2%           1:9.9\n",
      "  GAD    502       339       163     32.5%           1:2.1\n"
     ]
    }
   ],
   "source": [
    "# ==================== 4. 診斷分佈分析 ====================\n",
    "print(\"\\n[步驟 4] 診斷標籤分佈\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "label_distribution = []\n",
    "for label in LABEL_COLS:\n",
    "    if label in df.columns:\n",
    "        counts = df[label].value_counts(dropna=False)\n",
    "        total = df[label].notna().sum()\n",
    "        n_pos = counts.get(1, 0)\n",
    "        n_neg = counts.get(0, 0)\n",
    "        \n",
    "        label_distribution.append({\n",
    "            'label': label,\n",
    "            'total': total,\n",
    "            'negative': n_neg,\n",
    "            'positive': n_pos,\n",
    "            'pos_ratio': f\"{100*n_pos/total:.1f}%\" if total > 0 else \"0%\",\n",
    "            'imbalance_ratio': f\"1:{n_neg/n_pos:.1f}\" if n_pos > 0 else \"N/A\"\n",
    "        })\n",
    "\n",
    "dist_df = pd.DataFrame(label_distribution)\n",
    "print(dist_df.to_string(index=False))\n",
    "dist_df.to_csv(REPORTS_DIR / \"label_distribution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80f9911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 5] PCA 降維分析\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PCA 解釋變異量 (前10個主成分):\n",
      " PC  Variance_Ratio  Cumulative\n",
      "PC1        0.557316    0.557316\n",
      "PC2        0.163877    0.721193\n",
      "PC3        0.125902    0.847095\n",
      "PC4        0.077201    0.924296\n",
      "PC5        0.039472    0.963768\n",
      "PC6        0.022317    0.986085\n",
      "PC7        0.013915    1.000000\n",
      "✓ PCA 圖表已儲存\n",
      "✓ 已生成 4 個診斷的 PCA 散點圖\n"
     ]
    }
   ],
   "source": [
    "# ==================== 5. 改進的 PCA 分析 ====================\n",
    "print(\"\\n[步驟 5] PCA 降維分析\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "hrv_cols_available = [c for c in HRV_COLS if c in df.columns]\n",
    "X_pca = df[hrv_cols_available].copy()\n",
    "\n",
    "# KNN Imputer\n",
    "imp = KNNImputer(n_neighbors=5)\n",
    "X_pca_filled = pd.DataFrame(\n",
    "    imp.fit_transform(X_pca),\n",
    "    columns=X_pca.columns,\n",
    "    index=X_pca.index\n",
    ")\n",
    "\n",
    "# RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_pca_scaled = scaler.fit_transform(X_pca_filled)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(random_state=42)\n",
    "X_pca_transformed = pca.fit_transform(X_pca_scaled)\n",
    "\n",
    "# 解釋變異量\n",
    "explained_var_df = pd.DataFrame({\n",
    "    'PC': [f'PC{i+1}' for i in range(min(10, len(pca.explained_variance_ratio_)))],\n",
    "    'Variance_Ratio': pca.explained_variance_ratio_[:10],\n",
    "    'Cumulative': np.cumsum(pca.explained_variance_ratio_)[:10]\n",
    "})\n",
    "\n",
    "print(\"\\nPCA 解釋變異量 (前10個主成分):\")\n",
    "print(explained_var_df.to_string(index=False))\n",
    "\n",
    "# 視覺化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "n_show = min(15, len(pca.explained_variance_ratio_))\n",
    "ax1.bar(range(1, n_show+1), pca.explained_variance_ratio_[:n_show])\n",
    "ax1.set_xlabel('主成分編號', fontsize=11)\n",
    "ax1.set_ylabel('解釋變異量比例', fontsize=11)\n",
    "ax1.set_title('Scree Plot - 各主成分解釋變異量', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 累積解釋變異量\n",
    "ax2.plot(range(1, len(pca.explained_variance_ratio_)+1),\n",
    "         np.cumsum(pca.explained_variance_ratio_), 'bo-', linewidth=2)\n",
    "ax2.axhline(y=0.80, color='r', linestyle='--', label='80% 閾值', linewidth=2)\n",
    "ax2.axhline(y=0.90, color='orange', linestyle='--', label='90% 閾值', linewidth=2)\n",
    "ax2.set_xlabel('主成分數量', fontsize=11)\n",
    "ax2.set_ylabel('累積解釋變異量', fontsize=11)\n",
    "ax2.set_title('累積解釋變異量曲線', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / \"pca_variance_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ PCA 圖表已儲存\")\n",
    "\n",
    "# PCA 散點圖 (按診斷著色)\n",
    "for label in LABEL_COLS:\n",
    "    if label not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    mask0 = (df[label] == 0)\n",
    "    mask1 = (df[label] == 1)\n",
    "    \n",
    "    # PC1 vs PC2\n",
    "    axes[0].scatter(X_pca_transformed[mask0, 0], X_pca_transformed[mask0, 1],\n",
    "                   alpha=0.5, s=40, label=f'{label}=0 (n={mask0.sum()})')\n",
    "    axes[0].scatter(X_pca_transformed[mask1, 0], X_pca_transformed[mask1, 1],\n",
    "                   alpha=0.5, s=40, label=f'{label}=1 (n={mask1.sum()})')\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "    axes[0].set_title('PC1 vs PC2', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend(fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PC1 vs PC3\n",
    "    if len(pca.explained_variance_ratio_) >= 3:\n",
    "        axes[1].scatter(X_pca_transformed[mask0, 0], X_pca_transformed[mask0, 2],\n",
    "                       alpha=0.5, s=40, label=f'{label}=0')\n",
    "        axes[1].scatter(X_pca_transformed[mask1, 0], X_pca_transformed[mask1, 2],\n",
    "                       alpha=0.5, s=40, label=f'{label}=1')\n",
    "        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "        axes[1].set_ylabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})', fontsize=11)\n",
    "        axes[1].set_title('PC1 vs PC3', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend(fontsize=9)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # PC2 vs PC3\n",
    "        axes[2].scatter(X_pca_transformed[mask0, 1], X_pca_transformed[mask0, 2],\n",
    "                       alpha=0.5, s=40, label=f'{label}=0')\n",
    "        axes[2].scatter(X_pca_transformed[mask1, 1], X_pca_transformed[mask1, 2],\n",
    "                       alpha=0.5, s=40, label=f'{label}=1')\n",
    "        axes[2].set_xlabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "        axes[2].set_ylabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})', fontsize=11)\n",
    "        axes[2].set_title('PC2 vs PC3', fontsize=12, fontweight='bold')\n",
    "        axes[2].legend(fontsize=9)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'PCA 分析 - {label}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f\"pca_scatter_{label}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✓ 已生成 {len(LABEL_COLS)} 個診斷的 PCA 散點圖\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e521a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 6] 特徵重要性分析\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "分析 SSD 的特徵重要性...\n",
      "  Top 10 重要特徵:\n",
      "            feature  combined_score\n",
      "                BMI        0.777506\n",
      "               LFHF        0.734781\n",
      "Age_BMI_interaction        0.690302\n",
      "                 SC        0.601685\n",
      "                RSA        0.563007\n",
      "               SDNN        0.374005\n",
      "                Age        0.360884\n",
      "                 FT        0.345366\n",
      "        LF_HF_ratio        0.323478\n",
      "      SDNN_SC_ratio        0.285809\n",
      "\n",
      "分析 MDD 的特徵重要性...\n",
      "  Top 10 重要特徵:\n",
      "            feature  combined_score\n",
      "                Age        0.884025\n",
      "            HF_sqrt        0.820592\n",
      "                 HF        0.739043\n",
      "                 SC        0.734665\n",
      "        Age_squared        0.602606\n",
      "                 FT        0.573825\n",
      "                BMI        0.566743\n",
      "      SDNN_SC_ratio        0.555450\n",
      "         LF_plus_HF        0.518636\n",
      "Age_BMI_interaction        0.501345\n",
      "\n",
      "分析 Panic 的特徵重要性...\n",
      "  Top 10 重要特徵:\n",
      "    feature  combined_score\n",
      " LF_plus_HF        0.778273\n",
      "Age_squared        0.725566\n",
      "       LFHF        0.580140\n",
      "        Age        0.524691\n",
      "LF_minus_HF        0.503066\n",
      "         FT        0.500000\n",
      "LF_HF_ratio        0.482400\n",
      "        BMI        0.446982\n",
      "         SC        0.378157\n",
      "  SDNN_sqrt        0.337562\n",
      "\n",
      "分析 GAD 的特徵重要性...\n",
      "  Top 10 重要特徵:\n",
      "      feature  combined_score\n",
      "           FT        0.878458\n",
      "   LF_plus_HF        0.798576\n",
      "          RSA        0.785171\n",
      "         SDNN        0.738563\n",
      "           HF        0.623988\n",
      "    SDNN_sqrt        0.602457\n",
      "SDNN_SC_ratio        0.591585\n",
      "         LFHF        0.536111\n",
      "LF_SDNN_ratio        0.524408\n",
      "          BMI        0.517147\n",
      "\n",
      "✓ 特徵重要性分析完成\n"
     ]
    }
   ],
   "source": [
    "# ==================== 6. 特徵重要性分析 ====================\n",
    "print(\"\\n[步驟 6] 特徵重要性分析\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "feature_importance_results = {}\n",
    "\n",
    "for label in LABEL_COLS:\n",
    "    if label not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n分析 {label} 的特徵重要性...\")\n",
    "    \n",
    "    mask = df[label].notna()\n",
    "    available_features = [c for c in ALL_FEATURES_EXT if c in df.columns]\n",
    "    \n",
    "    X = df.loc[mask, available_features].copy()\n",
    "    y = df.loc[mask, label].values\n",
    "    \n",
    "    if len(np.unique(y)) < 2 or len(X) < 20:\n",
    "        print(f\"  資料不足，跳過\")\n",
    "        continue\n",
    "    \n",
    "    # KNN Imputer\n",
    "    imp = KNNImputer(n_neighbors=5)\n",
    "    X_filled = pd.DataFrame(imp.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Mutual Information\n",
    "    mi_scores = mutual_info_classif(X_filled, y, random_state=42)\n",
    "    \n",
    "    # Random Forest Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_filled, y)\n",
    "    rf_importance = rf.feature_importances_\n",
    "    \n",
    "    # 整合分數\n",
    "    mi_norm = mi_scores / (mi_scores.max() + 1e-9)\n",
    "    rf_norm = rf_importance / (rf_importance.max() + 1e-9)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'mutual_info': mi_scores,\n",
    "        'rf_importance': rf_importance,\n",
    "        'mi_normalized': mi_norm,\n",
    "        'rf_normalized': rf_norm,\n",
    "        'combined_score': (mi_norm + rf_norm) / 2\n",
    "    }).sort_values('combined_score', ascending=False)\n",
    "    \n",
    "    feature_importance_results[label] = importance_df\n",
    "    \n",
    "    print(f\"  Top 10 重要特徵:\")\n",
    "    print(importance_df.head(10)[['feature', 'combined_score']].to_string(index=False))\n",
    "    \n",
    "    # 儲存\n",
    "    importance_df.to_csv(REPORTS_DIR / f\"feature_importance_{label}.csv\", index=False)\n",
    "    \n",
    "    # 視覺化\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    top_n = min(15, len(importance_df))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    # Mutual Information\n",
    "    axes[0].barh(range(top_n), top_features['mutual_info'].values, color='steelblue')\n",
    "    axes[0].set_yticks(range(top_n))\n",
    "    axes[0].set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "    axes[0].set_xlabel('Mutual Information Score', fontsize=11)\n",
    "    axes[0].set_title(f'{label} - Mutual Information', fontsize=12, fontweight='bold')\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Random Forest\n",
    "    axes[1].barh(range(top_n), top_features['rf_importance'].values, color='coral')\n",
    "    axes[1].set_yticks(range(top_n))\n",
    "    axes[1].set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "    axes[1].set_xlabel('Random Forest Importance', fontsize=11)\n",
    "    axes[1].set_title(f'{label} - Random Forest', fontsize=12, fontweight='bold')\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f\"feature_importance_{label}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\n✓ 特徵重要性分析完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230bc587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 7] 訓練與評估機器學習模型\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "處理診斷標籤: SSD\n",
      "================================================================================\n",
      "✓ 總樣本數: 502\n",
      "✓ 正類比例: 24.70% (124/502)\n",
      "✓ 使用前 12 個重要特徵\n",
      "✓ 訓練集: 401, 測試集: 101\n",
      "✓ 不平衡比例: 1:3.1，使用 SMOTE\n",
      "✓ SMOTE 後樣本數: 401 → 604\n",
      "\n",
      "--- Logistic_L1 ---\n",
      "  CV AUC: 0.633 ± 0.044\n",
      "  Test AUC: 0.610\n",
      "  Test AP: 0.333\n",
      "  Test F1 (優化閾值): 0.459\n",
      "  過擬合程度: 0.041\n",
      "\n",
      "--- Logistic_L2 ---\n",
      "  CV AUC: 0.645 ± 0.024\n",
      "  Test AUC: 0.636\n",
      "  Test AP: 0.386\n",
      "  Test F1 (優化閾值): 0.483\n",
      "  過擬合程度: 0.035\n",
      "\n",
      "--- RandomForest_Simple ---\n",
      "  CV AUC: 0.784 ± 0.020\n",
      "  Test AUC: 0.646\n",
      "  Test AP: 0.436\n",
      "  Test F1 (優化閾值): 0.491\n",
      "  過擬合程度: 0.281\n",
      "\n",
      "--- GradientBoosting_Simple ---\n",
      "  CV AUC: 0.778 ± 0.027\n",
      "  Test AUC: 0.629\n",
      "  Test AP: 0.391\n",
      "  Test F1 (優化閾值): 0.468\n",
      "  過擬合程度: 0.342\n",
      "\n",
      "--- SVM_RBF ---\n",
      "  CV AUC: 0.756 ± 0.039\n",
      "  Test AUC: 0.618\n",
      "  Test AP: 0.349\n",
      "  Test F1 (優化閾值): 0.457\n",
      "  過擬合程度: 0.236\n",
      "\n",
      "================================================================================\n",
      "✓ 最佳模型: RandomForest_Simple\n",
      "  Test AUC: 0.646\n",
      "  Test AP: 0.436\n",
      "  Test F1: 0.491\n",
      "================================================================================\n",
      "✓ 模型已儲存: D:\\FLY114\\Analysis_Results_20251003_135729\\trained_models\\SSD_best_model.pkl\n",
      "\n",
      "================================================================================\n",
      "處理診斷標籤: MDD\n",
      "================================================================================\n",
      "✓ 總樣本數: 502\n",
      "✓ 正類比例: 19.92% (100/502)\n",
      "✓ 使用前 12 個重要特徵\n",
      "✓ 訓練集: 401, 測試集: 101\n",
      "✓ 不平衡比例: 1:4.0，使用 SMOTE\n",
      "✓ SMOTE 後樣本數: 401 → 642\n",
      "\n",
      "--- Logistic_L1 ---\n",
      "  CV AUC: 0.704 ± 0.033\n",
      "  Test AUC: 0.540\n",
      "  Test AP: 0.227\n",
      "  Test F1 (優化閾值): 0.377\n",
      "  過擬合程度: 0.183\n",
      "\n",
      "--- Logistic_L2 ---\n",
      "  CV AUC: 0.718 ± 0.033\n",
      "  Test AUC: 0.569\n",
      "  Test AP: 0.240\n",
      "  Test F1 (優化閾值): 0.382\n",
      "  過擬合程度: 0.169\n",
      "\n",
      "--- RandomForest_Simple ---\n",
      "  CV AUC: 0.857 ± 0.032\n",
      "  Test AUC: 0.592\n",
      "  Test AP: 0.244\n",
      "  Test F1 (優化閾值): 0.400\n",
      "  過擬合程度: 0.359\n",
      "\n",
      "--- GradientBoosting_Simple ---\n",
      "  CV AUC: 0.886 ± 0.032\n",
      "  Test AUC: 0.552\n",
      "  Test AP: 0.220\n",
      "  Test F1 (優化閾值): 0.385\n",
      "  過擬合程度: 0.431\n",
      "\n",
      "--- SVM_RBF ---\n",
      "  CV AUC: 0.839 ± 0.033\n",
      "  Test AUC: 0.601\n",
      "  Test AP: 0.259\n",
      "  Test F1 (優化閾值): 0.392\n",
      "  過擬合程度: 0.290\n",
      "\n",
      "================================================================================\n",
      "✓ 最佳模型: SVM_RBF\n",
      "  Test AUC: 0.601\n",
      "  Test AP: 0.259\n",
      "  Test F1: 0.392\n",
      "================================================================================\n",
      "✓ 模型已儲存: D:\\FLY114\\Analysis_Results_20251003_135729\\trained_models\\MDD_best_model.pkl\n",
      "\n",
      "================================================================================\n",
      "處理診斷標籤: Panic\n",
      "================================================================================\n",
      "✓ 總樣本數: 502\n",
      "✓ 正類比例: 9.16% (46/502)\n",
      "✓ 使用前 12 個重要特徵\n",
      "✓ 訓練集: 401, 測試集: 101\n",
      "✓ 不平衡比例: 1:9.8，使用 SMOTE\n",
      "✓ SMOTE 後樣本數: 401 → 728\n",
      "\n",
      "--- Logistic_L1 ---\n",
      "  CV AUC: 0.645 ± 0.018\n",
      "  Test AUC: 0.478\n",
      "  Test AP: 0.118\n",
      "  Test F1 (優化閾值): 0.273\n",
      "  過擬合程度: 0.189\n",
      "\n",
      "--- Logistic_L2 ---\n",
      "  CV AUC: 0.669 ± 0.025\n",
      "  Test AUC: 0.449\n",
      "  Test AP: 0.100\n",
      "  Test F1 (優化閾值): 0.178\n",
      "  過擬合程度: 0.238\n",
      "\n",
      "--- RandomForest_Simple ---\n",
      "  CV AUC: 0.881 ± 0.008\n",
      "  Test AUC: 0.603\n",
      "  Test AP: 0.148\n",
      "  Test F1 (優化閾值): 0.267\n",
      "  過擬合程度: 0.346\n",
      "\n",
      "--- GradientBoosting_Simple ---\n",
      "  CV AUC: 0.920 ± 0.008\n",
      "  Test AUC: 0.585\n",
      "  Test AP: 0.150\n",
      "  Test F1 (優化閾值): 0.250\n",
      "  過擬合程度: 0.399\n",
      "\n",
      "--- SVM_RBF ---\n",
      "  CV AUC: 0.868 ± 0.009\n",
      "  Test AUC: 0.545\n",
      "  Test AP: 0.145\n",
      "  Test F1 (優化閾值): 0.212\n",
      "  過擬合程度: 0.358\n",
      "\n",
      "================================================================================\n",
      "✓ 最佳模型: RandomForest_Simple\n",
      "  Test AUC: 0.603\n",
      "  Test AP: 0.148\n",
      "  Test F1: 0.267\n",
      "================================================================================\n",
      "✓ 模型已儲存: D:\\FLY114\\Analysis_Results_20251003_135729\\trained_models\\Panic_best_model.pkl\n",
      "\n",
      "================================================================================\n",
      "處理診斷標籤: GAD\n",
      "================================================================================\n",
      "✓ 總樣本數: 502\n",
      "✓ 正類比例: 32.47% (163/502)\n",
      "✓ 使用前 12 個重要特徵\n",
      "✓ 訓練集: 401, 測試集: 101\n",
      "✓ 不平衡比例: 1:2.1，使用 SMOTE\n",
      "✓ SMOTE 後樣本數: 401 → 542\n",
      "\n",
      "--- Logistic_L1 ---\n",
      "  CV AUC: 0.584 ± 0.008\n",
      "  Test AUC: 0.547\n",
      "  Test AP: 0.361\n",
      "  Test F1 (優化閾值): 0.504\n",
      "  過擬合程度: 0.089\n",
      "\n",
      "--- Logistic_L2 ---\n",
      "  CV AUC: 0.628 ± 0.037\n",
      "  Test AUC: 0.559\n",
      "  Test AP: 0.359\n",
      "  Test F1 (優化閾值): 0.512\n",
      "  過擬合程度: 0.108\n",
      "\n",
      "--- RandomForest_Simple ---\n",
      "  CV AUC: 0.709 ± 0.039\n",
      "  Test AUC: 0.566\n",
      "  Test AP: 0.433\n",
      "  Test F1 (優化閾值): 0.529\n",
      "  過擬合程度: 0.336\n",
      "\n",
      "--- GradientBoosting_Simple ---\n",
      "  CV AUC: 0.745 ± 0.039\n",
      "  Test AUC: 0.623\n",
      "  Test AP: 0.514\n",
      "  Test F1 (優化閾值): 0.523\n",
      "  過擬合程度: 0.343\n",
      "\n",
      "--- SVM_RBF ---\n",
      "  CV AUC: 0.685 ± 0.012\n",
      "  Test AUC: 0.592\n",
      "  Test AP: 0.439\n",
      "  Test F1 (優化閾值): 0.524\n",
      "  過擬合程度: 0.220\n",
      "\n",
      "================================================================================\n",
      "✓ 最佳模型: GradientBoosting_Simple\n",
      "  Test AUC: 0.623\n",
      "  Test AP: 0.514\n",
      "  Test F1: 0.523\n",
      "================================================================================\n",
      "✓ 模型已儲存: D:\\FLY114\\Analysis_Results_20251003_135729\\trained_models\\GAD_best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==================== 7. 進階機器學習模型 ====================\n",
    "print(\"\\n[步驟 7] 訓練與評估機器學習模型\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "def optimize_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    \"\"\"尋找最佳決策閾值\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    \n",
    "    if metric == 'f1':\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        best_idx = np.argmax(f1_scores[:-1])\n",
    "    elif metric == 'balanced':\n",
    "        # 平衡精確率和召回率\n",
    "        balanced_scores = np.sqrt(precision * recall)\n",
    "        best_idx = np.argmax(balanced_scores[:-1])\n",
    "    \n",
    "    return thresholds[best_idx] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "def comprehensive_evaluation(model, X_test, y_test, model_name, label_name, save_dir):\n",
    "    \"\"\"全面評估模型\"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 找最佳閾值\n",
    "    best_threshold = optimize_threshold(y_test, y_pred_proba, metric='f1')\n",
    "    \n",
    "    # 預測\n",
    "    y_pred_05 = (y_pred_proba >= 0.5).astype(int)\n",
    "    y_pred_opt = (y_pred_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    # 計算指標\n",
    "    metrics = {\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'ap': average_precision_score(y_test, y_pred_proba),\n",
    "        'f1_threshold_05': f1_score(y_test, y_pred_05),\n",
    "        'f1_threshold_opt': f1_score(y_test, y_pred_opt),\n",
    "        'best_threshold': best_threshold\n",
    "    }\n",
    "    \n",
    "    # 視覺化\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # ROC Curve\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    ax1.plot(fpr, tpr, linewidth=2, label=f'AUC = {metrics[\"auc\"]:.3f}')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=10)\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=10)\n",
    "    ax1.set_title('ROC Curve', fontsize=11, fontweight='bold')\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR Curve\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ax2.plot(recall, precision, linewidth=2, label=f'AP = {metrics[\"ap\"]:.3f}')\n",
    "    ax2.set_xlabel('Recall', fontsize=10)\n",
    "    ax2.set_ylabel('Precision', fontsize=10)\n",
    "    ax2.set_title('Precision-Recall Curve', fontsize=11, fontweight='bold')\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion Matrix (threshold = 0.5)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    cm_05 = confusion_matrix(y_test, y_pred_05)\n",
    "    sns.heatmap(cm_05, annot=True, fmt='d', cmap='Blues', ax=ax3, cbar=False)\n",
    "    ax3.set_xlabel('Predicted', fontsize=10)\n",
    "    ax3.set_ylabel('Actual', fontsize=10)\n",
    "    ax3.set_title('Confusion Matrix (threshold=0.5)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Confusion Matrix (optimal threshold)\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
    "    sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Greens', ax=ax4, cbar=False)\n",
    "    ax4.set_xlabel('Predicted', fontsize=10)\n",
    "    ax4.set_ylabel('Actual', fontsize=10)\n",
    "    ax4.set_title(f'Confusion Matrix (threshold={best_threshold:.3f})', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Prediction Distribution\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    ax5.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, label='Negative', density=True, color='blue')\n",
    "    ax5.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, label='Positive', density=True, color='red')\n",
    "    ax5.axvline(x=0.5, color='black', linestyle='--', linewidth=1, label='Default (0.5)')\n",
    "    ax5.axvline(x=best_threshold, color='green', linestyle='--', linewidth=2, label=f'Optimal ({best_threshold:.3f})')\n",
    "    ax5.set_xlabel('Predicted Probability', fontsize=10)\n",
    "    ax5.set_ylabel('Density', fontsize=10)\n",
    "    ax5.set_title('Prediction Distribution', fontsize=11, fontweight='bold')\n",
    "    ax5.legend(fontsize=8)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Threshold vs F1\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    thresholds_range = np.linspace(0.1, 0.9, 50)\n",
    "    f1_scores = []\n",
    "    for t in thresholds_range:\n",
    "        y_pred_t = (y_pred_proba >= t).astype(int)\n",
    "        f1_scores.append(f1_score(y_test, y_pred_t))\n",
    "    \n",
    "    ax6.plot(thresholds_range, f1_scores, linewidth=2)\n",
    "    ax6.axvline(x=best_threshold, color='r', linestyle='--', linewidth=2, label=f'Best = {best_threshold:.3f}')\n",
    "    ax6.set_xlabel('Threshold', fontsize=10)\n",
    "    ax6.set_ylabel('F1 Score', fontsize=10)\n",
    "    ax6.set_title('Threshold vs F1 Score', fontsize=11, fontweight='bold')\n",
    "    ax6.legend(fontsize=9)\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance Metrics Text\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    模型: {model_name} | 診斷標籤: {label_name} | 測試集大小: {len(y_test)}\n",
    "    \n",
    "    ROC-AUC: {metrics['auc']:.4f}\n",
    "    Average Precision: {metrics['ap']:.4f}\n",
    "    \n",
    "    F1 Score (threshold=0.5): {metrics['f1_threshold_05']:.4f}\n",
    "    F1 Score (threshold={best_threshold:.3f}): {metrics['f1_threshold_opt']:.4f}\n",
    "    \n",
    "    最佳閾值: {best_threshold:.4f}\n",
    "    \n",
    "    分類報告 (使用最佳閾值):\n",
    "    {classification_report(y_test, y_pred_opt, target_names=['Negative', 'Positive'], digits=3)}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax7.text(0.1, 0.5, metrics_text, fontsize=9, family='monospace',\n",
    "             verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - {label_name} 完整評估報告', \n",
    "                 fontsize=14, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.savefig(save_dir / f\"{label_name}_{model_name}_evaluation.png\",\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_improved_models(df, label, feature_cols, output_dir):\n",
    "    \"\"\"\n",
    "    訓練改進的模型 - 防過擬合版本\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"處理診斷標籤: {label}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 準備資料\n",
    "    mask = df[label].notna()\n",
    "    X = df.loc[mask, feature_cols].copy()\n",
    "    y = df.loc[mask, label].values.astype(int)\n",
    "    \n",
    "    if len(np.unique(y)) < 2 or len(X) < 50:\n",
    "        print(f\"⚠ 資料不足 (樣本數={len(X)})，跳過\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✓ 總樣本數: {len(X)}\")\n",
    "    print(f\"✓ 正類比例: {np.mean(y):.2%} ({np.sum(y)}/{len(y)})\")\n",
    "    \n",
    "    # KNN Imputer\n",
    "    imp = KNNImputer(n_neighbors=5)\n",
    "    X_filled = pd.DataFrame(imp.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # 特徵選擇 (使用前期分析的重要特徵)\n",
    "    if label in feature_importance_results:\n",
    "        top_features = feature_importance_results[label].head(12)['feature'].tolist()\n",
    "        selected_cols = [c for c in top_features if c in X_filled.columns]\n",
    "        \n",
    "        if len(selected_cols) >= 5:\n",
    "            X_filled = X_filled[selected_cols]\n",
    "            print(f\"✓ 使用前 {len(selected_cols)} 個重要特徵\")\n",
    "    \n",
    "    # 分割資料\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_filled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ 訓練集: {len(X_train)}, 測試集: {len(X_test)}\")\n",
    "    \n",
    "    # 標準化 (使用 RobustScaler)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # SMOTE (如果需要)\n",
    "    imbalance_ratio = np.sum(y_train == 0) / max(np.sum(y_train == 1), 1)\n",
    "    \n",
    "    if imbalance_ratio > 2 and np.sum(y_train == 1) >= 6:\n",
    "        print(f\"✓ 不平衡比例: 1:{imbalance_ratio:.1f}，使用 SMOTE\")\n",
    "        \n",
    "        try:\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(5, np.sum(y_train == 1) - 1))\n",
    "            X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "            print(f\"✓ SMOTE 後樣本數: {len(y_train)} → {len(y_train_balanced)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ SMOTE 失敗: {e}，使用原始資料\")\n",
    "            X_train_balanced, y_train_balanced = X_train_scaled, y_train\n",
    "    else:\n",
    "        X_train_balanced, y_train_balanced = X_train_scaled, y_train\n",
    "    \n",
    "    # 定義模型 (防過擬合配置)\n",
    "    models = {\n",
    "        'Logistic_L1': LogisticRegression(\n",
    "            penalty='l1',\n",
    "            C=0.1,\n",
    "            solver='saga',\n",
    "            max_iter=3000,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'Logistic_L2': LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=1.0,\n",
    "            solver='lbfgs',\n",
    "            max_iter=3000,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'RandomForest_Simple': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'GradientBoosting_Simple': GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'SVM_RBF': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # 訓練和評估所有模型\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        \n",
    "        # 交叉驗證\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_validate(\n",
    "            model, X_train_balanced, y_train_balanced,\n",
    "            cv=cv,\n",
    "            scoring={\n",
    "                'roc_auc': 'roc_auc',\n",
    "                'average_precision': 'average_precision',\n",
    "                'f1': 'f1'\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # 訓練最終模型\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        \n",
    "        # 測試集評估\n",
    "        test_metrics = comprehensive_evaluation(\n",
    "            model, X_test_scaled, y_test,\n",
    "            model_name, label, output_dir\n",
    "        )\n",
    "        \n",
    "        # 整合結果\n",
    "        results[model_name] = {\n",
    "            'cv_auc_mean': np.mean(cv_scores['test_roc_auc']),\n",
    "            'cv_auc_std': np.std(cv_scores['test_roc_auc']),\n",
    "            'cv_ap_mean': np.mean(cv_scores['test_average_precision']),\n",
    "            'cv_f1_mean': np.mean(cv_scores['test_f1']),\n",
    "            'train_auc_mean': np.mean(cv_scores['train_roc_auc']),\n",
    "            'test_auc': test_metrics['auc'],\n",
    "            'test_ap': test_metrics['ap'],\n",
    "            'test_f1_opt': test_metrics['f1_threshold_opt'],\n",
    "            'best_threshold': test_metrics['best_threshold'],\n",
    "            'overfitting_gap': np.mean(cv_scores['train_roc_auc']) - test_metrics['auc']\n",
    "        }\n",
    "        \n",
    "        print(f\"  CV AUC: {results[model_name]['cv_auc_mean']:.3f} ± {results[model_name]['cv_auc_std']:.3f}\")\n",
    "        print(f\"  Test AUC: {test_metrics['auc']:.3f}\")\n",
    "        print(f\"  Test AP: {test_metrics['ap']:.3f}\")\n",
    "        print(f\"  Test F1 (優化閾值): {test_metrics['f1_threshold_opt']:.3f}\")\n",
    "        print(f\"  過擬合程度: {results[model_name]['overfitting_gap']:.3f}\")\n",
    "    \n",
    "    # 找最佳模型\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['test_auc'])[0]\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✓ 最佳模型: {best_model_name}\")\n",
    "    print(f\"  Test AUC: {results[best_model_name]['test_auc']:.3f}\")\n",
    "    print(f\"  Test AP: {results[best_model_name]['test_ap']:.3f}\")\n",
    "    print(f\"  Test F1: {results[best_model_name]['test_f1_opt']:.3f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 儲存最佳模型\n",
    "    model_save_path = MODELS_DIR / f\"{label}_best_model.pkl\"\n",
    "    joblib.dump({\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'imputer': imp,\n",
    "        'feature_names': X_filled.columns.tolist(),\n",
    "        'threshold': results[best_model_name]['best_threshold']\n",
    "    }, model_save_path)\n",
    "    \n",
    "    print(f\"✓ 模型已儲存: {model_save_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 對每個診斷執行分析\n",
    "all_results = {}\n",
    "\n",
    "for label in LABEL_COLS:\n",
    "    if label not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    available_features = [c for c in ALL_FEATURES_EXT if c in df.columns]\n",
    "    \n",
    "    results = train_improved_models(df, label, available_features, PLOTS_DIR)\n",
    "    \n",
    "    if results:\n",
    "        all_results[label] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ea2169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 8] 結果彙整與報告生成\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "完整模型比較結果:\n",
      "Label                   Model CV_AUC_Mean CV_AUC_Std Test_AUC Test_AP Test_F1 Best_Threshold Overfitting_Gap\n",
      "  SSD             Logistic_L1       0.633      0.044    0.610   0.333   0.459          0.541           0.041\n",
      "  SSD             Logistic_L2       0.645      0.024    0.636   0.386   0.483          0.587           0.035\n",
      "  SSD     RandomForest_Simple       0.784      0.020    0.646   0.436   0.491          0.578           0.281\n",
      "  SSD GradientBoosting_Simple       0.778      0.027    0.629   0.391   0.468          0.477           0.342\n",
      "  SSD                 SVM_RBF       0.756      0.039    0.618   0.349   0.457          0.253           0.236\n",
      "  MDD             Logistic_L1       0.704      0.033    0.540   0.227   0.377          0.565           0.183\n",
      "  MDD             Logistic_L2       0.718      0.033    0.569   0.240   0.382          0.483           0.169\n",
      "  MDD     RandomForest_Simple       0.857      0.032    0.592   0.244   0.400          0.340           0.359\n",
      "  MDD GradientBoosting_Simple       0.886      0.032    0.552   0.220   0.385          0.286           0.431\n",
      "  MDD                 SVM_RBF       0.839      0.033    0.601   0.259   0.392          0.510           0.290\n",
      "Panic             Logistic_L1       0.645      0.018    0.478   0.118   0.273          0.610           0.189\n",
      "Panic             Logistic_L2       0.669      0.025    0.449   0.100   0.178          0.297           0.238\n",
      "Panic     RandomForest_Simple       0.881      0.008    0.603   0.148   0.267          0.573           0.346\n",
      "Panic GradientBoosting_Simple       0.920      0.008    0.585   0.150   0.250          0.726           0.399\n",
      "Panic                 SVM_RBF       0.868      0.009    0.545   0.145   0.212          0.159           0.358\n",
      "  GAD             Logistic_L1       0.584      0.008    0.547   0.361   0.504          0.347           0.089\n",
      "  GAD             Logistic_L2       0.628      0.037    0.559   0.359   0.512          0.181           0.108\n",
      "  GAD     RandomForest_Simple       0.709      0.039    0.566   0.433   0.529          0.507           0.336\n",
      "  GAD GradientBoosting_Simple       0.745      0.039    0.623   0.514   0.523          0.415           0.343\n",
      "  GAD                 SVM_RBF       0.685      0.012    0.592   0.439   0.524          0.259           0.220\n",
      "\n",
      "最佳模型摘要:\n",
      "Label              Best_Model Test_AUC Test_AP Test_F1        CV_AUC Overfitting\n",
      "  SSD     RandomForest_Simple    0.646   0.436   0.491 0.784 ± 0.020       0.281\n",
      "  MDD                 SVM_RBF    0.601   0.259   0.392 0.839 ± 0.033       0.290\n",
      "Panic     RandomForest_Simple    0.603   0.148   0.267 0.881 ± 0.008       0.346\n",
      "  GAD GradientBoosting_Simple    0.623   0.514   0.523 0.745 ± 0.039       0.343\n",
      "\n",
      "✓ 比較圖表已儲存\n"
     ]
    }
   ],
   "source": [
    "# ==================== 8. 結果彙整 ====================\n",
    "print(\"\\n[步驟 8] 結果彙整與報告生成\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 建立比較表格\n",
    "comparison_rows = []\n",
    "for label, model_results in all_results.items():\n",
    "    for model_name, metrics in model_results.items():\n",
    "        comparison_rows.append({\n",
    "            'Label': label,\n",
    "            'Model': model_name,\n",
    "            'CV_AUC_Mean': f\"{metrics['cv_auc_mean']:.3f}\",\n",
    "            'CV_AUC_Std': f\"{metrics['cv_auc_std']:.3f}\",\n",
    "            'Test_AUC': f\"{metrics['test_auc']:.3f}\",\n",
    "            'Test_AP': f\"{metrics['test_ap']:.3f}\",\n",
    "            'Test_F1': f\"{metrics['test_f1_opt']:.3f}\",\n",
    "            'Best_Threshold': f\"{metrics['best_threshold']:.3f}\",\n",
    "            'Overfitting_Gap': f\"{metrics['overfitting_gap']:.3f}\"\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "\n",
    "print(\"\\n完整模型比較結果:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 儲存比較結果\n",
    "comparison_df.to_csv(REPORTS_DIR / \"model_comparison_full.csv\", index=False)\n",
    "comparison_df.to_excel(OUT_DIR / \"model_comparison.xlsx\", index=False, engine='openpyxl')\n",
    "\n",
    "# 生成最佳模型摘要\n",
    "best_models_summary = []\n",
    "for label in all_results.keys():\n",
    "    results = all_results[label]\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['test_auc'])\n",
    "    \n",
    "    best_models_summary.append({\n",
    "        'Label': label,\n",
    "        'Best_Model': best_model[0],\n",
    "        'Test_AUC': f\"{best_model[1]['test_auc']:.3f}\",\n",
    "        'Test_AP': f\"{best_model[1]['test_ap']:.3f}\",\n",
    "        'Test_F1': f\"{best_model[1]['test_f1_opt']:.3f}\",\n",
    "        'CV_AUC': f\"{best_model[1]['cv_auc_mean']:.3f} ± {best_model[1]['cv_auc_std']:.3f}\",\n",
    "        'Overfitting': f\"{best_model[1]['overfitting_gap']:.3f}\"\n",
    "    })\n",
    "\n",
    "best_summary_df = pd.DataFrame(best_models_summary)\n",
    "\n",
    "print(\"\\n最佳模型摘要:\")\n",
    "print(best_summary_df.to_string(index=False))\n",
    "\n",
    "best_summary_df.to_csv(REPORTS_DIR / \"best_models_summary.csv\", index=False)\n",
    "\n",
    "# 視覺化最佳模型比較\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "labels_list = list(all_results.keys())\n",
    "metrics_to_plot = ['test_auc', 'test_ap', 'test_f1_opt', 'overfitting_gap']\n",
    "titles = ['Test AUC', 'Test AP (Average Precision)', 'Test F1 (Optimized)', 'Overfitting Gap']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen', 'indianred']\n",
    "\n",
    "for idx, (metric, title, color) in enumerate(zip(metrics_to_plot, titles, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # 取得每個標籤的所有模型該指標\n",
    "    for label_idx, label in enumerate(labels_list):\n",
    "        results = all_results[label]\n",
    "        model_names = list(results.keys())\n",
    "        metric_values = [results[m][metric] for m in model_names]\n",
    "        \n",
    "        x_positions = np.arange(len(model_names)) + label_idx * 0.15\n",
    "        ax.bar(x_positions, metric_values, width=0.15, \n",
    "               label=label, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Models', fontsize=11)\n",
    "    ax.set_ylabel(title, fontsize=11)\n",
    "    ax.set_title(f'{title} - 所有診斷比較', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(np.arange(len(model_names)) + 0.225)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=9)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    if metric != 'overfitting_gap':\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / \"all_models_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n✓ 比較圖表已儲存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ea7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[步驟 9] 生成最終分析報告\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "機器學習分析完整報告\n",
      "====================================================================================================\n",
      "\n",
      "執行時間: 2025-10-03 13:58:02\n",
      "資料來源: D:\\FLY114\\Diagnosis and autonomic marker data for VNS research_20250813.xlsx\n",
      "\n",
      "====================================================================================================\n",
      "1. 資料概況\n",
      "====================================================================================================\n",
      "\n",
      "原始樣本數: 502\n",
      "特徵總數: 23\n",
      "  - 基本特徵: 3\n",
      "  - 控制變數: 3\n",
      "  - HRV 特徵: 7\n",
      "  - 工程特徵: 10\n",
      "\n",
      "診斷標籤分佈:\n",
      "label  total  negative  positive pos_ratio imbalance_ratio\n",
      "  SSD    502       378       124     24.7%           1:3.0\n",
      "  MDD    502       402       100     19.9%           1:4.0\n",
      "Panic    502       456        46      9.2%           1:9.9\n",
      "  GAD    502       339       163     32.5%           1:2.1\n",
      "\n",
      "====================================================================================================\n",
      "2. 最佳模型表現\n",
      "====================================================================================================\n",
      "\n",
      "Label              Best_Model Test_AUC Test_AP Test_F1        CV_AUC Overfitting\n",
      "  SSD     RandomForest_Simple    0.646   0.436   0.491 0.784 ± 0.020       0.281\n",
      "  MDD                 SVM_RBF    0.601   0.259   0.392 0.839 ± 0.033       0.290\n",
      "Panic     RandomForest_Simple    0.603   0.148   0.267 0.881 ± 0.008       0.346\n",
      "  GAD GradientBoosting_Simple    0.623   0.514   0.523 0.745 ± 0.039       0.343\n",
      "\n",
      "====================================================================================================\n",
      "3. 關鍵發現\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "SSD:\n",
      "  ✓ 最佳模型: RandomForest_Simple\n",
      "  ✓ Test AUC: 0.646\n",
      "  ✓ Test AP: 0.436\n",
      "  ✓ Test F1: 0.491\n",
      "  ✓ 最佳閾值: 0.578\n",
      "  ✓ 過擬合程度: 0.281\n",
      "  \n",
      "  前5重要特徵:\n",
      "    BMI: 0.778\n",
      "    LFHF: 0.735\n",
      "    Age_BMI_interaction: 0.690\n",
      "    SC: 0.602\n",
      "    RSA: 0.563\n",
      "\n",
      "\n",
      "MDD:\n",
      "  ✓ 最佳模型: SVM_RBF\n",
      "  ✓ Test AUC: 0.601\n",
      "  ✓ Test AP: 0.259\n",
      "  ✓ Test F1: 0.392\n",
      "  ✓ 最佳閾值: 0.510\n",
      "  ✓ 過擬合程度: 0.290\n",
      "  \n",
      "  前5重要特徵:\n",
      "    Age: 0.884\n",
      "    HF_sqrt: 0.821\n",
      "    HF: 0.739\n",
      "    SC: 0.735\n",
      "    Age_squared: 0.603\n",
      "\n",
      "\n",
      "Panic:\n",
      "  ✓ 最佳模型: RandomForest_Simple\n",
      "  ✓ Test AUC: 0.603\n",
      "  ✓ Test AP: 0.148\n",
      "  ✓ Test F1: 0.267\n",
      "  ✓ 最佳閾值: 0.573\n",
      "  ✓ 過擬合程度: 0.346\n",
      "  \n",
      "  前5重要特徵:\n",
      "    LF_plus_HF: 0.778\n",
      "    Age_squared: 0.726\n",
      "    LFHF: 0.580\n",
      "    Age: 0.525\n",
      "    LF_minus_HF: 0.503\n",
      "\n",
      "\n",
      "GAD:\n",
      "  ✓ 最佳模型: GradientBoosting_Simple\n",
      "  ✓ Test AUC: 0.623\n",
      "  ✓ Test AP: 0.514\n",
      "  ✓ Test F1: 0.523\n",
      "  ✓ 最佳閾值: 0.415\n",
      "  ✓ 過擬合程度: 0.343\n",
      "  \n",
      "  前5重要特徵:\n",
      "    FT: 0.878\n",
      "    LF_plus_HF: 0.799\n",
      "    RSA: 0.785\n",
      "    SDNN: 0.739\n",
      "    HF: 0.624\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "4. 改善建議\n",
      "====================================================================================================\n",
      "\n",
      "基於分析結果，以下是針對每個診斷的具體建議:\n",
      "\n",
      "SSD:\n",
      "  ⚠ 表現中等 (AUC=0.646)，建議收集更多資料或新增特徵\n",
      "  ⚠ 有過擬合傾向 (差距=0.281)，建議增加正則化或減少特徵\n",
      "\n",
      "MDD:\n",
      "  ⚠ 表現中等 (AUC=0.601)，建議收集更多資料或新增特徵\n",
      "  ⚠ 有過擬合傾向 (差距=0.290)，建議增加正則化或減少特徵\n",
      "\n",
      "Panic:\n",
      "  ⚠ 表現中等 (AUC=0.603)，建議收集更多資料或新增特徵\n",
      "  ⚠ 有過擬合傾向 (差距=0.346)，建議增加正則化或減少特徵\n",
      "\n",
      "GAD:\n",
      "  ⚠ 表現中等 (AUC=0.623)，建議收集更多資料或新增特徵\n",
      "  ⚠ 有過擬合傾向 (差距=0.343)，建議增加正則化或減少特徵\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "5. 輸出檔案\n",
      "====================================================================================================\n",
      "\n",
      "報告位置: D:\\FLY114\\Analysis_Results_20251003_135729\\reports\n",
      "圖表位置: D:\\FLY114\\Analysis_Results_20251003_135729\\plots\n",
      "模型位置: D:\\FLY114\\Analysis_Results_20251003_135729\\trained_models\n",
      "\n",
      "主要輸出:\n",
      "  - model_comparison.xlsx: 完整模型比較\n",
      "  - best_models_summary.csv: 最佳模型摘要\n",
      "  - feature_importance_*.csv: 各診斷的特徵重要性\n",
      "  - *_evaluation.png: 詳細評估圖表\n",
      "  - *_best_model.pkl: 訓練好的模型\n",
      "\n",
      "====================================================================================================\n",
      "分析完成!\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "✓ 完整報告已儲存: D:\\FLY114\\Analysis_Results_20251003_135729\\reports\\analysis_report.txt\n",
      "\n",
      "[步驟 10] 模型使用說明\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "如何使用訓練好的模型進行預測:\n",
      "\n",
      "```python\n",
      "import joblib\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# 1. 載入模型\n",
      "model_data = joblib.load('trained_models/GAD_best_model.pkl')\n",
      "model = model_data['model']\n",
      "scaler = model_data['scaler']\n",
      "imputer = model_data['imputer']\n",
      "feature_names = model_data['feature_names']\n",
      "threshold = model_data['threshold']\n",
      "\n",
      "# 2. 準備新資料\n",
      "new_data = pd.DataFrame({\n",
      "    'Age': [45],\n",
      "    'Sex': [1],\n",
      "    'BMI': [24.5],\n",
      "    # ... 其他特徵\n",
      "})\n",
      "\n",
      "# 3. 確保特徵順序一致\n",
      "new_data = new_data[feature_names]\n",
      "\n",
      "# 4. 填補缺失值\n",
      "new_data_imputed = imputer.transform(new_data)\n",
      "\n",
      "# 5. 標準化\n",
      "new_data_scaled = scaler.transform(new_data_imputed)\n",
      "\n",
      "# 6. 預測\n",
      "pred_proba = model.predict_proba(new_data_scaled)[:, 1]\n",
      "pred_class = (pred_proba >= threshold).astype(int)\n",
      "\n",
      "print(f\"預測機率: {pred_proba[0]:.3f}\")\n",
      "print(f\"預測類別: {'陽性' if pred_class[0] == 1 else '陰性'}\")\n",
      "```\n",
      "\n",
      "\n",
      "✓ 使用說明已儲存: D:\\FLY114\\Analysis_Results_20251003_135729\\reports\\model_usage_guide.txt\n",
      "\n",
      "====================================================================================================\n",
      "🎉 分析流程完成!\n",
      "====================================================================================================\n",
      "\n",
      "所有結果已儲存至: D:\\FLY114\\Analysis_Results_20251003_135729\n",
      "  📊 圖表: D:\\FLY114\\Analysis_Results_20251003_135729\\plots\n",
      "  🤖 模型: D:\\FLY114\\Analysis_Results_20251003_135729\\trained_models\n",
      "  📄 報告: D:\\FLY114\\Analysis_Results_20251003_135729\\reports\n",
      "\n",
      "感謝使用!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== 9. 生成最終報告 ====================\n",
    "print(\"\\n[步驟 9] 生成最終分析報告\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "report_content = f\"\"\"\n",
    "{'='*100}\n",
    "機器學習分析完整報告\n",
    "{'='*100}\n",
    "\n",
    "執行時間: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "資料來源: {XLSX_PATH}\n",
    "\n",
    "{'='*100}\n",
    "1. 資料概況\n",
    "{'='*100}\n",
    "\n",
    "原始樣本數: {len(df)}\n",
    "特徵總數: {len(ALL_FEATURES_EXT)}\n",
    "  - 基本特徵: {len(BASIC_COLS)}\n",
    "  - 控制變數: {len(CONTROL_COLS)}\n",
    "  - HRV 特徵: {len(HRV_COLS)}\n",
    "  - 工程特徵: {len(engineered_features)}\n",
    "\n",
    "診斷標籤分佈:\n",
    "{dist_df.to_string(index=False)}\n",
    "\n",
    "{'='*100}\n",
    "2. 最佳模型表現\n",
    "{'='*100}\n",
    "\n",
    "{best_summary_df.to_string(index=False)}\n",
    "\n",
    "{'='*100}\n",
    "3. 關鍵發現\n",
    "{'='*100}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 添加每個診斷的關鍵發現\n",
    "for label in all_results.keys():\n",
    "    results = all_results[label]\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['test_auc'])\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "{label}:\n",
    "  ✓ 最佳模型: {best_model[0]}\n",
    "  ✓ Test AUC: {best_model[1]['test_auc']:.3f}\n",
    "  ✓ Test AP: {best_model[1]['test_ap']:.3f}\n",
    "  ✓ Test F1: {best_model[1]['test_f1_opt']:.3f}\n",
    "  ✓ 最佳閾值: {best_model[1]['best_threshold']:.3f}\n",
    "  ✓ 過擬合程度: {best_model[1]['overfitting_gap']:.3f}\n",
    "  \n",
    "  前5重要特徵:\n",
    "\"\"\"\n",
    "    \n",
    "    if label in feature_importance_results:\n",
    "        top_5 = feature_importance_results[label].head(5)\n",
    "        for i, row in top_5.iterrows():\n",
    "            report_content += f\"    {row['feature']}: {row['combined_score']:.3f}\\n\"\n",
    "    \n",
    "    report_content += \"\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "{'='*100}\n",
    "4. 改善建議\n",
    "{'='*100}\n",
    "\n",
    "基於分析結果，以下是針對每個診斷的具體建議:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for label in all_results.keys():\n",
    "    results = all_results[label]\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['test_auc'])\n",
    "    \n",
    "    auc = best_model[1]['test_auc']\n",
    "    gap = best_model[1]['overfitting_gap']\n",
    "    \n",
    "    report_content += f\"{label}:\\n\"\n",
    "    \n",
    "    if auc >= 0.7:\n",
    "        report_content += f\"  ✓ 表現良好 (AUC={auc:.3f})\\n\"\n",
    "    elif auc >= 0.6:\n",
    "        report_content += f\"  ⚠ 表現中等 (AUC={auc:.3f})，建議收集更多資料或新增特徵\\n\"\n",
    "    else:\n",
    "        report_content += f\"  ✗ 表現較差 (AUC={auc:.3f})，需要重新評估特徵或資料品質\\n\"\n",
    "    \n",
    "    if gap > 0.15:\n",
    "        report_content += f\"  ⚠ 有過擬合傾向 (差距={gap:.3f})，建議增加正則化或減少特徵\\n\"\n",
    "    else:\n",
    "        report_content += f\"  ✓ 過擬合控制良好 (差距={gap:.3f})\\n\"\n",
    "    \n",
    "    report_content += \"\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "{'='*100}\n",
    "5. 輸出檔案\n",
    "{'='*100}\n",
    "\n",
    "報告位置: {REPORTS_DIR}\n",
    "圖表位置: {PLOTS_DIR}\n",
    "模型位置: {MODELS_DIR}\n",
    "\n",
    "主要輸出:\n",
    "  - model_comparison.xlsx: 完整模型比較\n",
    "  - best_models_summary.csv: 最佳模型摘要\n",
    "  - feature_importance_*.csv: 各診斷的特徵重要性\n",
    "  - *_evaluation.png: 詳細評估圖表\n",
    "  - *_best_model.pkl: 訓練好的模型\n",
    "\n",
    "{'='*100}\n",
    "分析完成!\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "# 儲存報告\n",
    "report_path = REPORTS_DIR / \"analysis_report.txt\"\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(report_content)\n",
    "print(f\"\\n✓ 完整報告已儲存: {report_path}\")\n",
    "\n",
    "# ==================== 10. 如何使用訓練好的模型進行預測 ====================\n",
    "print(\"\\n[步驟 10] 模型使用說明\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "usage_guide = \"\"\"\n",
    "如何使用訓練好的模型進行預測:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 載入模型\n",
    "model_data = joblib.load('trained_models/GAD_best_model.pkl')\n",
    "model = model_data['model']\n",
    "scaler = model_data['scaler']\n",
    "imputer = model_data['imputer']\n",
    "feature_names = model_data['feature_names']\n",
    "threshold = model_data['threshold']\n",
    "\n",
    "# 2. 準備新資料\n",
    "new_data = pd.DataFrame({\n",
    "    'Age': [45],\n",
    "    'Sex': [1],\n",
    "    'BMI': [24.5],\n",
    "    # ... 其他特徵\n",
    "})\n",
    "\n",
    "# 3. 確保特徵順序一致\n",
    "new_data = new_data[feature_names]\n",
    "\n",
    "# 4. 填補缺失值\n",
    "new_data_imputed = imputer.transform(new_data)\n",
    "\n",
    "# 5. 標準化\n",
    "new_data_scaled = scaler.transform(new_data_imputed)\n",
    "\n",
    "# 6. 預測\n",
    "pred_proba = model.predict_proba(new_data_scaled)[:, 1]\n",
    "pred_class = (pred_proba >= threshold).astype(int)\n",
    "\n",
    "print(f\"預測機率: {pred_proba[0]:.3f}\")\n",
    "print(f\"預測類別: {'陽性' if pred_class[0] == 1 else '陰性'}\")\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(usage_guide)\n",
    "\n",
    "usage_path = REPORTS_DIR / \"model_usage_guide.txt\"\n",
    "with open(usage_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(usage_guide)\n",
    "\n",
    "print(f\"\\n✓ 使用說明已儲存: {usage_path}\")\n",
    "\n",
    "# ==================== 完成 ====================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🎉 分析流程完成!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n所有結果已儲存至: {OUT_DIR}\")\n",
    "print(f\"  📊 圖表: {PLOTS_DIR}\")\n",
    "print(f\"  🤖 模型: {MODELS_DIR}\")\n",
    "print(f\"  📄 報告: {REPORTS_DIR}\")\n",
    "print(\"\\n感謝使用!\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrv-eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
